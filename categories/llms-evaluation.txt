# LLMS EVALUATION (9 repositories)

bytedance/web-bench: Web-Bench is a benchmark designed to evaluate the performance of LLMs in actual Web development.  - https://github.com/bytedance/web-bench
ShishirPatil/gorilla: Gorilla: Training and Evaluating LLMs for Function Calls (Tool Calls) - https://github.com/ShishirPatil/gorilla
ammaarreshi/Gemini-Search: Perplexity style AI Search engine clone built with Gemini 2.0 Flash and Grounding - https://github.com/ammaarreshi/Gemini-Search
arcprize/arc-agi-benchmarking: Testing baseline LLMs performance across various models - https://github.com/arcprize/arc-agi-benchmarking
The-FinAI/PIXIU: This repository introduces PIXIU, an open-source resource featuring the first financial large language models (LLMs), instruction tuning data, and evaluation benchmarks to holistically assess financial LLMs. Our goal is to continually push forward the open-source development of financial artificial intelligence (AI). - https://github.com/The-FinAI/PIXIU
ScalingIntelligence/KernelBench: KernelBench: Can LLMs Write GPU Kernels? - Benchmark with Torch -> CUDA problems - https://github.com/ScalingIntelligence/KernelBench
JackHopkins/factorio-learning-environment: A non-saturating, open-ended environment for evaluating LLMs in Factorio - https://github.com/JackHopkins/factorio-learning-environment
tensorzero/tensorzero: TensorZero is an open-source stack for industrial-grade LLM applications. It unifies an LLM gateway, observability, optimization, evaluation, and experimentation. - https://github.com/tensorzero/tensorzero
magicproduct/hash-hop: Long context evaluation for large language models - https://github.com/magicproduct/hash-hop
