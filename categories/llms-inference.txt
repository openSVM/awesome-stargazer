# LLMS INFERENCE (8 repositories)

PrimeIntellect-ai/prime-vllm: Modded vLLM to run pipeline parallelism over public networks - https://github.com/PrimeIntellect-ai/prime-vllm
vllm-project/aibrix: Cost-efficient and pluggable Infrastructure components for GenAI inference - https://github.com/vllm-project/aibrix
France-Travail/happy_vllm: A REST API for vLLM, production ready - https://github.com/France-Travail/happy_vllm
JackYFL/awesome-VLLMs: This repository collects papers on VLLM applications. We will update new papers irregularly. - https://github.com/JackYFL/awesome-VLLMs
interestingLSY/swiftLLM: A tiny yet powerful LLM inference system tailored for researching purpose. vLLM-equivalent performance with only 2k lines of code (2% of vLLM). - https://github.com/interestingLSY/swiftLLM
vllm-project/vllm: A high-throughput and memory-efficient inference and serving engine for LLMs - https://github.com/vllm-project/vllm
vllm-project/production-stack: vLLMâ€™s reference system for K8S-native cluster-wide deployment with community-driven performance optimization - https://github.com/vllm-project/production-stack
NetEase-Media/grps: Deep Learning Deployment Framework: Supports tf/torch/trt/trtllm/vllm and other NN frameworks. Support dynamic batching, and streaming modes. It is dual-language compatible with Python and C++, offering scalability, extensibility, and high performance. It helps users quickly deploy models and provide services through HTTP/RPC interfaces. - https://github.com/NetEase-Media/grps
