# LLMS OPENSOURCE MODELS (41 repositories)

sammcj/gollama: Go manage your Ollama models - https://github.com/sammcj/gollama
ggml-org/llama.cpp: LLM inference in C/C++ - https://github.com/ggml-org/llama.cpp
intel/ipex-llm: Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, DeepSeek, Mixtral, Gemma, Phi, MiniCPM, Qwen-VL, MiniCPM-V, etc.) on Intel XPU (e.g., local PC with iGPU and NPU, discrete GPU such as Arc, Flex and Max); seamlessly integrate with llama.cpp, Ollama, HuggingFace, LangChain, LlamaIndex, vLLM, DeepSpeed, Axolotl, etc. - https://github.com/intel/ipex-llm
e2b-dev/ai-analyst: Open source AI analyst powered by E2B. Analyze your CSV files with Llama 3.1 and create interactive charts. - https://github.com/e2b-dev/ai-analyst
varunvasudeva1/llm-server-docs: Documentation on setting up a local LLM server on Debian from scratch, using Ollama/llama.cpp/vLLM, Open WebUI, Kokoro FastAPI, and ComfyUI. - https://github.com/varunvasudeva1/llm-server-docs
felladrin/MiniSearch: Minimalist web-searching platform with an AI assistant that runs directly from your browser. Uses WebLLM, Wllama and SearXNG. Demo: https://felladrin-minisearch.hf.space - https://github.com/felladrin/MiniSearch
zhihu/ZhiLight: A highly optimized LLM inference acceleration engine for Llama and its variants. - https://github.com/zhihu/ZhiLight
Mozilla-Ocho/llamafile: Distribute and run LLMs with a single file. - https://github.com/Mozilla-Ocho/llamafile
h1ddenpr0cess20/ollamarama-matrix: AI chatbot for Matrix with infinite personalties, using ollama - https://github.com/h1ddenpr0cess20/ollamarama-matrix
jzhang38/TinyLlama: The TinyLlama project is an open endeavor to pretrain a 1.1B Llama model on 3 trillion tokens. - https://github.com/jzhang38/TinyLlama
guinmoon/LLMFarm: llama and other  large language models on iOS and MacOS offline using GGML library. - https://github.com/guinmoon/LLMFarm
ngxson/wllama: WebAssembly binding for llama.cpp - Enabling on-browser LLM inference - https://github.com/ngxson/wllama
RahulSChand/gpu_poor: Calculate token/s & GPU memory requirement for any LLM.  Supports llama.cpp/ggml/bnb/QLoRA quantization - https://github.com/RahulSChand/gpu_poor
KhoomeiK/LlamaGym: Fine-tune LLM agents with online reinforcement learning - https://github.com/KhoomeiK/LlamaGym
sauravpanda/BrowserAI: Run local LLMs like llama, deepseek-distill, kokoro and more inside your browser - https://github.com/sauravpanda/BrowserAI
QwenLM/Qwen: The official repo of Qwen (通义千问) chat & pretrained large language model proposed by Alibaba Cloud. - https://github.com/QwenLM/Qwen
modelscope/ms-swift: Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, GLM4.5, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen3-VL, Qwen3-Omni, InternVL3.5, Ovis2.5, Llava, GLM4v, Phi4, ...) (AAAI 2025). - https://github.com/modelscope/ms-swift
tusharhero/aitelegrambot: aitelegrambot is a telegram bot which uses Ollama as its backend. - https://github.com/tusharhero/aitelegrambot
tcsenpai/DualMind: DualMind is an innovative AI conversation simulator that facilitates engaging dialogues between two AI models using the Ollama API. - https://github.com/tcsenpai/DualMind
rlondner/LangChain-for-LLM-Application-Development: In LangChain for LLM Application Development, you will gain essential skills in expanding the use cases and capabilities of language models in application development using the LangChain framework. Includes code compatible with Ollama, ChromaDB and Pinecone. - https://github.com/rlondner/LangChain-for-LLM-Application-Development
run-llama/LlamaIndexTS: Data framework for your LLM applications. Focus on server side solution - https://github.com/run-llama/LlamaIndexTS
run-llama/llama_index: LlamaIndex is the leading framework for building LLM-powered agents over your data. - https://github.com/run-llama/llama_index
run-llama/create-llama: The easiest way to get started with LlamaIndex - https://github.com/run-llama/create-llama
EricLBuehler/mistral.rs: Blazingly fast LLM inference. - https://github.com/EricLBuehler/mistral.rs
run-llama/ts-agents: Demonstration of agentic capabilities in TypeScript - https://github.com/run-llama/ts-agents
OneInterface/realtime-bakllava: llama.cpp with BakLLaVA model describes what does it see - https://github.com/OneInterface/realtime-bakllava
kelindar/search: Go library for embedded vector search and semantic embeddings using llama.cpp - https://github.com/kelindar/search
hiyouga/LLaMA-Factory: Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024) - https://github.com/hiyouga/LLaMA-Factory
QwenLM/Qwen3-VL: Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud. - https://github.com/QwenLM/Qwen3-VL
QwenLM/Qwen3: Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud. - https://github.com/QwenLM/Qwen3
b4rtaz/distributed-llama: Distributed LLM inference. Connect home devices into a powerful cluster to accelerate LLM inference. More devices means faster inference. - https://github.com/b4rtaz/distributed-llama
ollama-cloud/ollama-as-wasm-plugin: Get up and running with Llama 2, Mistral, Gemma, and other large language models. - https://github.com/ollama-cloud/ollama-as-wasm-plugin
ollama-cloud/get-started: Ollama Cloud is a Highly Scalable Cloud-native Stack for Ollama - https://github.com/ollama-cloud/get-started
johnbean393/Sidekick: A native macOS app that allows users to chat with a local LLM that can respond with information from files, folders and websites on your Mac without installing any other software. Powered by llama.cpp. - https://github.com/johnbean393/Sidekick
cgbur/llama2.zig: Inference Llama 2 in one file of pure Zig - https://github.com/cgbur/llama2.zig
llamastack/llama-stack-apps: Agentic components of the Llama Stack APIs - https://github.com/llamastack/llama-stack-apps
xNul/code-llama-for-vscode: Use Code Llama with Visual Studio Code and the Continue extension. A local LLM alternative to GitHub Copilot. - https://github.com/xNul/code-llama-for-vscode
spirobel/bunny-llama: iterate quickly with llama.cpp hot reloading. use the llama.cpp bindings with bun.sh - https://github.com/spirobel/bunny-llama
run-llama/ts-playground:  - https://github.com/run-llama/ts-playground
meta-llama/llama3: The official Meta Llama 3 GitHub site - https://github.com/meta-llama/llama3
Go_Projects_gollama: 1 repos
