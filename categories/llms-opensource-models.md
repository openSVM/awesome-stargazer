[â† Previous: Llms Inference](llms-inference.txt) | [ğŸ  Back to README](../README.md) | [Next: Llms Other 1 â†’](llms-other-1.txt)

---

# LLMS OPENSOURCE MODELS

**41 repositories**

---

## [sammcj/gollama](https://github.com/sammcj/gollama)

Go manage your Ollama models

ğŸ”— [https://github.com/sammcj/gollama](https://github.com/sammcj/gollama)

---

## [ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)

LLM inference in C/C++

ğŸ”— [https://github.com/ggml-org/llama.cpp](https://github.com/ggml-org/llama.cpp)

---

## [intel/ipex-llm](https://github.com/intel/ipex-llm)

Accelerate local LLM inference and finetuning (LLaMA, Mistral, ChatGLM, Qwen, DeepSeek, Mixtral, Gemma, Phi, MiniCPM, Qwen-VL, MiniCPM-V, etc.) on Intel XPU (e.g., local PC with iGPU and NPU, discrete GPU such as Arc, Flex and Max); seamlessly integrate with llama.cpp, Ollama, HuggingFace, LangChain, LlamaIndex, vLLM, DeepSpeed, Axolotl, etc.

ğŸ”— [https://github.com/intel/ipex-llm](https://github.com/intel/ipex-llm)

---

## [e2b-dev/ai-analyst](https://github.com/e2b-dev/ai-analyst)

Open source AI analyst powered by E2B. Analyze your CSV files with Llama 3.1 and create interactive charts.

ğŸ”— [https://github.com/e2b-dev/ai-analyst](https://github.com/e2b-dev/ai-analyst)

---

## [varunvasudeva1/llm-server-docs](https://github.com/varunvasudeva1/llm-server-docs)

Documentation on setting up a local LLM server on Debian from scratch, using Ollama/llama.cpp/vLLM, Open WebUI, Kokoro FastAPI, and ComfyUI.

ğŸ”— [https://github.com/varunvasudeva1/llm-server-docs](https://github.com/varunvasudeva1/llm-server-docs)

---

## [felladrin/MiniSearch](https://github.com/felladrin/MiniSearch)

Minimalist web-searching platform with an AI assistant that runs directly from your browser. Uses WebLLM, Wllama and SearXNG. Demo: https://felladrin-minisearch.hf.space

ğŸ”— [https://github.com/felladrin/MiniSearch](https://github.com/felladrin/MiniSearch)

---

## [zhihu/ZhiLight](https://github.com/zhihu/ZhiLight)

A highly optimized LLM inference acceleration engine for Llama and its variants.

ğŸ”— [https://github.com/zhihu/ZhiLight](https://github.com/zhihu/ZhiLight)

---

## [Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile)

Distribute and run LLMs with a single file.

ğŸ”— [https://github.com/Mozilla-Ocho/llamafile](https://github.com/Mozilla-Ocho/llamafile)

---

## [h1ddenpr0cess20/ollamarama-matrix](https://github.com/h1ddenpr0cess20/ollamarama-matrix)

AI chatbot for Matrix with infinite personalties, using ollama

ğŸ”— [https://github.com/h1ddenpr0cess20/ollamarama-matrix](https://github.com/h1ddenpr0cess20/ollamarama-matrix)

---

## [jzhang38/TinyLlama](https://github.com/jzhang38/TinyLlama)

The TinyLlama project is an open endeavor to pretrain a 1.1B Llama model on 3 trillion tokens.

ğŸ”— [https://github.com/jzhang38/TinyLlama](https://github.com/jzhang38/TinyLlama)

---

## [guinmoon/LLMFarm](https://github.com/guinmoon/LLMFarm)

llama and other  large language models on iOS and MacOS offline using GGML library.

ğŸ”— [https://github.com/guinmoon/LLMFarm](https://github.com/guinmoon/LLMFarm)

---

## [ngxson/wllama](https://github.com/ngxson/wllama)

WebAssembly binding for llama.cpp - Enabling on-browser LLM inference

ğŸ”— [https://github.com/ngxson/wllama](https://github.com/ngxson/wllama)

---

## [RahulSChand/gpu_poor](https://github.com/RahulSChand/gpu_poor)

Calculate token/s & GPU memory requirement for any LLM.  Supports llama.cpp/ggml/bnb/QLoRA quantization

ğŸ”— [https://github.com/RahulSChand/gpu_poor](https://github.com/RahulSChand/gpu_poor)

---

## [KhoomeiK/LlamaGym](https://github.com/KhoomeiK/LlamaGym)

Fine-tune LLM agents with online reinforcement learning

ğŸ”— [https://github.com/KhoomeiK/LlamaGym](https://github.com/KhoomeiK/LlamaGym)

---

## [sauravpanda/BrowserAI](https://github.com/sauravpanda/BrowserAI)

Run local LLMs like llama, deepseek-distill, kokoro and more inside your browser

ğŸ”— [https://github.com/sauravpanda/BrowserAI](https://github.com/sauravpanda/BrowserAI)

---

## [QwenLM/Qwen](https://github.com/QwenLM/Qwen)

The official repo of Qwen (é€šä¹‰åƒé—®) chat & pretrained large language model proposed by Alibaba Cloud.

ğŸ”— [https://github.com/QwenLM/Qwen](https://github.com/QwenLM/Qwen)

---

## [modelscope/ms-swift](https://github.com/modelscope/ms-swift)

Use PEFT or Full-parameter to CPT/SFT/DPO/GRPO 500+ LLMs (Qwen3, Qwen3-MoE, Llama4, GLM4.5, InternLM3, DeepSeek-R1, ...) and 200+ MLLMs (Qwen3-VL, Qwen3-Omni, InternVL3.5, Ovis2.5, Llava, GLM4v, Phi4, ...) (AAAI 2025).

ğŸ”— [https://github.com/modelscope/ms-swift](https://github.com/modelscope/ms-swift)

---

## [tusharhero/aitelegrambot](https://github.com/tusharhero/aitelegrambot)

aitelegrambot is a telegram bot which uses Ollama as its backend.

ğŸ”— [https://github.com/tusharhero/aitelegrambot](https://github.com/tusharhero/aitelegrambot)

---

## [tcsenpai/DualMind](https://github.com/tcsenpai/DualMind)

DualMind is an innovative AI conversation simulator that facilitates engaging dialogues between two AI models using the Ollama API.

ğŸ”— [https://github.com/tcsenpai/DualMind](https://github.com/tcsenpai/DualMind)

---

## [rlondner/LangChain-for-LLM-Application-Development](https://github.com/rlondner/LangChain-for-LLM-Application-Development)

In LangChain for LLM Application Development, you will gain essential skills in expanding the use cases and capabilities of language models in application development using the LangChain framework. Includes code compatible with Ollama, ChromaDB and Pinecone.

ğŸ”— [https://github.com/rlondner/LangChain-for-LLM-Application-Development](https://github.com/rlondner/LangChain-for-LLM-Application-Development)

---

## [run-llama/LlamaIndexTS](https://github.com/run-llama/LlamaIndexTS)

Data framework for your LLM applications. Focus on server side solution

ğŸ”— [https://github.com/run-llama/LlamaIndexTS](https://github.com/run-llama/LlamaIndexTS)

---

## [run-llama/llama_index](https://github.com/run-llama/llama_index)

LlamaIndex is the leading framework for building LLM-powered agents over your data.

ğŸ”— [https://github.com/run-llama/llama_index](https://github.com/run-llama/llama_index)

---

## [run-llama/create-llama](https://github.com/run-llama/create-llama)

The easiest way to get started with LlamaIndex

ğŸ”— [https://github.com/run-llama/create-llama](https://github.com/run-llama/create-llama)

---

## [EricLBuehler/mistral.rs](https://github.com/EricLBuehler/mistral.rs)

Blazingly fast LLM inference.

ğŸ”— [https://github.com/EricLBuehler/mistral.rs](https://github.com/EricLBuehler/mistral.rs)

---

## [run-llama/ts-agents](https://github.com/run-llama/ts-agents)

Demonstration of agentic capabilities in TypeScript

ğŸ”— [https://github.com/run-llama/ts-agents](https://github.com/run-llama/ts-agents)

---

## [OneInterface/realtime-bakllava](https://github.com/OneInterface/realtime-bakllava)

llama.cpp with BakLLaVA model describes what does it see

ğŸ”— [https://github.com/OneInterface/realtime-bakllava](https://github.com/OneInterface/realtime-bakllava)

---

## [kelindar/search](https://github.com/kelindar/search)

Go library for embedded vector search and semantic embeddings using llama.cpp

ğŸ”— [https://github.com/kelindar/search](https://github.com/kelindar/search)

---

## [hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

Unified Efficient Fine-Tuning of 100+ LLMs & VLMs (ACL 2024)

ğŸ”— [https://github.com/hiyouga/LLaMA-Factory](https://github.com/hiyouga/LLaMA-Factory)

---

## [QwenLM/Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)

Qwen3-VL is the multimodal large language model series developed by Qwen team, Alibaba Cloud.

ğŸ”— [https://github.com/QwenLM/Qwen3-VL](https://github.com/QwenLM/Qwen3-VL)

---

## [QwenLM/Qwen3](https://github.com/QwenLM/Qwen3)

Qwen3 is the large language model series developed by Qwen team, Alibaba Cloud.

ğŸ”— [https://github.com/QwenLM/Qwen3](https://github.com/QwenLM/Qwen3)

---

## [b4rtaz/distributed-llama](https://github.com/b4rtaz/distributed-llama)

Distributed LLM inference. Connect home devices into a powerful cluster to accelerate LLM inference. More devices means faster inference.

ğŸ”— [https://github.com/b4rtaz/distributed-llama](https://github.com/b4rtaz/distributed-llama)

---

## [ollama-cloud/ollama-as-wasm-plugin](https://github.com/ollama-cloud/ollama-as-wasm-plugin)

Get up and running with Llama 2, Mistral, Gemma, and other large language models.

ğŸ”— [https://github.com/ollama-cloud/ollama-as-wasm-plugin](https://github.com/ollama-cloud/ollama-as-wasm-plugin)

---

## [ollama-cloud/get-started](https://github.com/ollama-cloud/get-started)

Ollama Cloud is a Highly Scalable Cloud-native Stack for Ollama

ğŸ”— [https://github.com/ollama-cloud/get-started](https://github.com/ollama-cloud/get-started)

---

## [johnbean393/Sidekick](https://github.com/johnbean393/Sidekick)

A native macOS app that allows users to chat with a local LLM that can respond with information from files, folders and websites on your Mac without installing any other software. Powered by llama.cpp.

ğŸ”— [https://github.com/johnbean393/Sidekick](https://github.com/johnbean393/Sidekick)

---

## [cgbur/llama2.zig](https://github.com/cgbur/llama2.zig)

Inference Llama 2 in one file of pure Zig

ğŸ”— [https://github.com/cgbur/llama2.zig](https://github.com/cgbur/llama2.zig)

---

## [llamastack/llama-stack-apps](https://github.com/llamastack/llama-stack-apps)

Agentic components of the Llama Stack APIs

ğŸ”— [https://github.com/llamastack/llama-stack-apps](https://github.com/llamastack/llama-stack-apps)

---

## [xNul/code-llama-for-vscode](https://github.com/xNul/code-llama-for-vscode)

Use Code Llama with Visual Studio Code and the Continue extension. A local LLM alternative to GitHub Copilot.

ğŸ”— [https://github.com/xNul/code-llama-for-vscode](https://github.com/xNul/code-llama-for-vscode)

---

## [spirobel/bunny-llama](https://github.com/spirobel/bunny-llama)

iterate quickly with llama.cpp hot reloading. use the llama.cpp bindings with bun.sh

ğŸ”— [https://github.com/spirobel/bunny-llama](https://github.com/spirobel/bunny-llama)

---

## [run-llama/ts-playground](https://github.com/run-llama/ts-playground)



ğŸ”— [https://github.com/run-llama/ts-playground](https://github.com/run-llama/ts-playground)

---

## [meta-llama/llama3](https://github.com/meta-llama/llama3)

The official Meta Llama 3 GitHub site

ğŸ”— [https://github.com/meta-llama/llama3](https://github.com/meta-llama/llama3)

---


[â† Previous: Llms Inference](llms-inference.txt) | [ğŸ  Back to README](../README.md) | [Next: Llms Other 1 â†’](llms-other-1.txt)
